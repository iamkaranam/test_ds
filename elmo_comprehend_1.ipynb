{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pooling...\n",
      "Pooling Finished...\n",
      "   fbcomment_id sentiment                                               text  \\\n",
      "0    9065432149   NEUTRAL          We would upgrade to the A380 anyway! Haha   \n",
      "1    8841832192  NEGATIVE  \"They understand this and have no problem reso...   \n",
      "2    8841832194  NEGATIVE  #topthreenotsocustomerservicemoments After bum...   \n",
      "3    8841832196   NEUTRAL          Hey Laurie, can you send both please? ^EH   \n",
      "4    8967296634  NEGATIVE  I am so incredibly disappointed with my servic...   \n",
      "\n",
      "   senti_lable                                             tokens  \n",
      "0            2                     [would, upgrade, anyway, haha]  \n",
      "1            0  [understand, problem, resolving, baggage, issu...  \n",
      "2            0  [topthreenotsocustomerservicemoments, bumping,...  \n",
      "3            2                        [hey, laurie, send, please]  \n",
      "4            0  [incredibly, disappointed, service, united, fa...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# this was a main branch line but we added some more text after creating a new branch branch_1 while staying in branch branch_1\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers import Activation\n",
    "from scipy.special import softmax\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "\n",
    "def amazon_lable(x):\n",
    "    a=-1\n",
    "    if(x=='POSITIVE'):\n",
    "        a = 1\n",
    "    if(x=='NEGATIVE'):\n",
    "        a = 0\n",
    "    if(x=='NEUTRAL'):\n",
    "        a = 2\n",
    "    return a\n",
    "\n",
    "def extract_words(temp):\n",
    "    #temp = re.sub(r'[^\\w\\s]','',temp) #punctuation\n",
    "    temp = re.sub(r'[^a-zA-Z0-9\\s]', ' ', temp)\n",
    "    temp = re.sub(r'\\d+','',temp) #remove numbers\n",
    "    temp = temp.lower()\n",
    "    temp = tb(temp)\n",
    "    lista = [word for word in temp.words if word not in stopwords.words('english')]\n",
    "    listb = lista\n",
    "    listb = [lemmatizer.lemmatize(y) for y in lista]\n",
    "    #listb = [stemmer.stem(y) for y in listb]\n",
    "    listb = [word for word in listb if len(word)>2]\n",
    "    #listb = list(set(listb))\n",
    "    return(listb)\n",
    "\n",
    "#not necessary for elmo algo\n",
    "# def buildWordVector(tokens, size):\n",
    "#     vec = np.zeros(size).reshape((1, size))\n",
    "#     count = 0.\n",
    "#     for word in tokens:\n",
    "#         try:\n",
    "#             vec += google_w2vec[word].reshape((1, size))\n",
    "#             count += 1.\n",
    "#         except KeyError: # handling the case where the token is not\n",
    "#                          # in the corpus. useful for testing.  * tfidf[word]\n",
    "#             continue\n",
    "#     if count != 0:\n",
    "#         vec /= count\n",
    "#     return vec\n",
    "\n",
    "def get_lable(b):\n",
    "    a = list(b)\n",
    "    i = a.index(max(a))\n",
    "    if(i==0):\n",
    "        lable = 0\n",
    "    if(i==1):\n",
    "        lable = 1\n",
    "    if(i==2):\n",
    "        lable = 2\n",
    "    return lable\n",
    "\n",
    "def compare(x1,x2):\n",
    "    if(x1==x2):\n",
    "        c = 0\n",
    "    else:\n",
    "        c = 1\n",
    "    return c\n",
    "\n",
    "def get_precision(x):\n",
    "    df = x\n",
    "    print(\"Precision for negatives:\" +str(df.iloc[0][0]/sum(df[0])))\n",
    "    print(\"Precision for positives:\" + str(df.iloc[1][1]/sum(df[1])))\n",
    "    print(\"Precision for neutrals:\" + str(df.iloc[2][2]/sum(df[2])))\n",
    "\n",
    "connection = pymysql.connect(host='ec2-174-129-19-167.compute-1.amazonaws.com',\n",
    "    \t\t\t\t\t\t\t user='umtdev',\n",
    "    \t\t\t\t\t\t\t password='GX^2M$4%781!ra1t-e~',\n",
    "    \t\t\t\t\t\t\t db='unmetric',\n",
    "    \t\t\t\t\t\t\t charset='utf8mb4',\n",
    "    \t\t\t\t\t\t\t cursorclass=pymysql.cursors.DictCursor,autocommit = True)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "query = str(\"select a.fbcomment_id,a.text,b.sentiment from fb_comment_text a join\\\n",
    "            fb_sentiment_comments_comprehend b on b.fbpagepostcomments_id = a.fbcomment_id;\")\n",
    "request = cursor.execute(query)\n",
    "comments_amazon = pd.DataFrame(cursor.fetchall())\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "comments_amazon['senti_lable'] = comments_amazon['sentiment'].apply(lambda x : amazon_lable(x))\n",
    "\n",
    "pool = Pool(processes=cpu_count()-1)   \n",
    "print(\"Starting Pooling...\")\n",
    "results =  pool.map(extract_words, comments_amazon['text'])\n",
    "print(\"Pooling Finished...\")\n",
    "pool.close()\n",
    "pool.join()\n",
    "comments_amazon['tokens'] = results\n",
    "print(comments_amazon.head())\n",
    "\n",
    "data = comments_amazon[{'tokens','senti_lable','fbcomment_id'}]\n",
    "data['clean_comment'] = data['tokens'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "ids_train = pd.read_csv(\"amazon_train_ids.csv\")\n",
    "\n",
    "ids_train = np.array(ids_train['0'])\n",
    "\n",
    "data_train = data[data.fbcomment_id.isin(ids_train)]\n",
    "\n",
    "#x_train = np.array(data_train.tokens)\n",
    "\n",
    "y_train = np.array(data_train.senti_lable)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "ids_test = pd.read_csv(\"amazon_test_ids.csv\")\n",
    "\n",
    "ids_test = np.array(ids_test['0'])\n",
    "\n",
    "data_test = data[data.fbcomment_id.isin(ids_test)]\n",
    "\n",
    "#x_test = np.array(data_test.tokens)\n",
    "\n",
    "y_test = np.array(data_test.senti_lable)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"default\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = [data_test[i:i+1000] for i in range(0,data_test.shape[0],1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train = [data_train[i:i+1000] for i in range(0,data_train.shape[0],1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_train = [None] * len(list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,90):\n",
    "    print(i)\n",
    "    x = list_train[i]\n",
    "    elmo_train[i] = elmo_vectors(x['clean_comment'])\n",
    "    if(i%20==0 or i==89):\n",
    "        pickle_out = open(\"elmo_train_comprehend_default1.pickle\",\"wb\")\n",
    "        pickle.dump(elmo_train, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"elmo_train_comprehend.pickle\",\"wb\")\n",
    "pickle.dump(elmo_train, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_test = [None] * len(list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4,30):\n",
    "    print(i)\n",
    "    x = list_test[i]\n",
    "    elmo_test[i] = elmo_vectors(x['clean_comment'])\n",
    "    if(i%20==0 or i==29):\n",
    "        pickle_out = open(\"elmo_test_comprehend.pickle\",\"wb\")\n",
    "        pickle.dump(elmo_test, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load elmo training data\n",
    "pickle_in = open(\"elmo_train_comprehend_all.pickle\", \"rb\")\n",
    "elmo_train_total = pickle.load(pickle_in)\n",
    "elmo_train_total = np.concatenate(elmo_train_total, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "#load elmo test data\n",
    "pickle_in = open(\"elmo_test_comprehend.pickle\", \"rb\")\n",
    "elmo_test_comprehend = pickle.load(pickle_in)\n",
    "elmo_test_comprehend_all = [None]*len(elmo_test_comprehend)\n",
    "elmo_test_comprehend_all[0:30] = elmo_test_comprehend[0:30]\n",
    "pickle_in = open(\"elmo_test_comprehend2.pickle\", \"rb\")\n",
    "elmo_test_comprehend2 = pickle.load(pickle_in)\n",
    "elmo_test_comprehend_all[30:len(elmo_test_comprehend2)] = elmo_test_comprehend2[30:len(elmo_test_comprehend2)]\n",
    "elmo_test_total = np.concatenate(elmo_test_comprehend_all, axis = 0)\n",
    "elmo_test_scaled = scale(elmo_test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"elmo_test_comprehend_all.pickle\",\"wb\")\n",
    "pickle.dump(elmo_test_total, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='elu', input_dim=1024))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "elmo_train_scaled = scale(elmo_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240447 samples, validate on 26717 samples\n",
      "Epoch 1/50\n",
      " - 37s - loss: 0.3704 - acc: 0.8419 - val_loss: 0.3837 - val_acc: 0.8417\n",
      "Epoch 2/50\n",
      " - 36s - loss: 0.3421 - acc: 0.8544 - val_loss: 0.3520 - val_acc: 0.8466\n",
      "Epoch 3/50\n",
      " - 35s - loss: 0.3342 - acc: 0.8581 - val_loss: 0.3420 - val_acc: 0.8492\n",
      "Epoch 4/50\n",
      " - 36s - loss: 0.3292 - acc: 0.8603 - val_loss: 0.3413 - val_acc: 0.8508\n",
      "Epoch 5/50\n",
      " - 35s - loss: 0.3248 - acc: 0.8616 - val_loss: 0.3440 - val_acc: 0.8474\n",
      "Epoch 6/50\n",
      " - 36s - loss: 0.3212 - acc: 0.8636 - val_loss: 0.3456 - val_acc: 0.8495\n",
      "Epoch 7/50\n",
      " - 36s - loss: 0.3184 - acc: 0.8648 - val_loss: 0.3445 - val_acc: 0.8441\n",
      "Epoch 8/50\n",
      " - 36s - loss: 0.3166 - acc: 0.8652 - val_loss: 0.3449 - val_acc: 0.8494\n",
      "Epoch 9/50\n",
      " - 35s - loss: 0.3145 - acc: 0.8665 - val_loss: 0.3337 - val_acc: 0.8544\n",
      "Epoch 10/50\n",
      " - 36s - loss: 0.3144 - acc: 0.8664 - val_loss: 0.3348 - val_acc: 0.8537\n",
      "Epoch 11/50\n",
      " - 36s - loss: 0.3114 - acc: 0.8679 - val_loss: 0.3354 - val_acc: 0.8531\n",
      "Epoch 12/50\n",
      " - 37s - loss: 0.3102 - acc: 0.8685 - val_loss: 0.3306 - val_acc: 0.8562\n",
      "Epoch 13/50\n",
      " - 37s - loss: 0.3087 - acc: 0.8688 - val_loss: 0.4066 - val_acc: 0.8187\n",
      "Epoch 14/50\n",
      " - 37s - loss: 0.3091 - acc: 0.8690 - val_loss: 0.3326 - val_acc: 0.8532\n",
      "Epoch 15/50\n",
      " - 37s - loss: 0.3056 - acc: 0.8706 - val_loss: 0.3376 - val_acc: 0.8542\n",
      "Epoch 16/50\n",
      " - 37s - loss: 0.3060 - acc: 0.8701 - val_loss: 0.3383 - val_acc: 0.8539\n",
      "Epoch 17/50\n",
      " - 36s - loss: 0.3044 - acc: 0.8713 - val_loss: 0.3309 - val_acc: 0.8557\n",
      "Epoch 18/50\n",
      " - 36s - loss: 0.3022 - acc: 0.8725 - val_loss: 0.3386 - val_acc: 0.8509\n",
      "Epoch 19/50\n",
      " - 36s - loss: 0.3019 - acc: 0.8721 - val_loss: 0.3364 - val_acc: 0.8541\n",
      "Epoch 20/50\n",
      " - 37s - loss: 0.3008 - acc: 0.8727 - val_loss: 0.3332 - val_acc: 0.8549\n",
      "Epoch 21/50\n",
      " - 37s - loss: 0.2995 - acc: 0.8735 - val_loss: 0.3318 - val_acc: 0.8553\n",
      "Epoch 22/50\n",
      " - 37s - loss: 0.3008 - acc: 0.8731 - val_loss: 0.3320 - val_acc: 0.8557\n",
      "Epoch 23/50\n",
      " - 37s - loss: 0.3015 - acc: 0.8737 - val_loss: 0.3369 - val_acc: 0.8539\n",
      "Epoch 24/50\n",
      " - 36s - loss: 0.2979 - acc: 0.8743 - val_loss: 0.3332 - val_acc: 0.8550\n",
      "Epoch 25/50\n",
      " - 37s - loss: 0.2983 - acc: 0.8742 - val_loss: 0.3327 - val_acc: 0.8544\n",
      "Epoch 26/50\n",
      " - 37s - loss: 0.2980 - acc: 0.8745 - val_loss: 0.3458 - val_acc: 0.8542\n",
      "Epoch 27/50\n",
      " - 36s - loss: 0.2988 - acc: 0.8748 - val_loss: 0.3316 - val_acc: 0.8564\n",
      "Epoch 28/50\n",
      " - 37s - loss: 0.2962 - acc: 0.8755 - val_loss: 0.3347 - val_acc: 0.8558\n",
      "Epoch 29/50\n",
      " - 37s - loss: 0.2949 - acc: 0.8757 - val_loss: 0.3313 - val_acc: 0.8555\n",
      "Epoch 30/50\n",
      " - 37s - loss: 0.2988 - acc: 0.8739 - val_loss: 0.3325 - val_acc: 0.8574\n",
      "Epoch 31/50\n",
      " - 37s - loss: 0.2937 - acc: 0.8760 - val_loss: 0.3514 - val_acc: 0.8471\n",
      "Epoch 32/50\n",
      " - 37s - loss: 0.2946 - acc: 0.8764 - val_loss: 0.3325 - val_acc: 0.8545\n",
      "Epoch 33/50\n",
      " - 36s - loss: 0.2935 - acc: 0.8767 - val_loss: 0.3376 - val_acc: 0.8533\n",
      "Epoch 34/50\n",
      " - 37s - loss: 0.2919 - acc: 0.8771 - val_loss: 0.3318 - val_acc: 0.8559\n",
      "Epoch 35/50\n",
      " - 37s - loss: 0.2915 - acc: 0.8773 - val_loss: 0.3347 - val_acc: 0.8556\n",
      "Epoch 36/50\n",
      " - 37s - loss: 0.2906 - acc: 0.8783 - val_loss: 0.3335 - val_acc: 0.8538\n",
      "Epoch 37/50\n",
      " - 37s - loss: 0.2908 - acc: 0.8776 - val_loss: 0.3483 - val_acc: 0.8538\n",
      "Epoch 38/50\n",
      " - 36s - loss: 0.2896 - acc: 0.8781 - val_loss: 0.3376 - val_acc: 0.8537\n",
      "Epoch 39/50\n",
      " - 37s - loss: 0.2916 - acc: 0.8781 - val_loss: 0.3355 - val_acc: 0.8546\n",
      "Epoch 40/50\n",
      " - 37s - loss: 0.3007 - acc: 0.8750 - val_loss: 0.3544 - val_acc: 0.8475\n",
      "Epoch 41/50\n",
      " - 37s - loss: 0.2907 - acc: 0.8780 - val_loss: 0.3352 - val_acc: 0.8557\n",
      "Epoch 42/50\n",
      " - 37s - loss: 0.2875 - acc: 0.8793 - val_loss: 0.3335 - val_acc: 0.8548\n",
      "Epoch 43/50\n",
      " - 37s - loss: 0.2875 - acc: 0.8793 - val_loss: 0.3332 - val_acc: 0.8556\n",
      "Epoch 44/50\n",
      " - 36s - loss: 0.2872 - acc: 0.8798 - val_loss: 0.3359 - val_acc: 0.8563\n",
      "Epoch 45/50\n",
      " - 37s - loss: 0.2864 - acc: 0.8801 - val_loss: 0.3373 - val_acc: 0.8502\n",
      "Epoch 46/50\n",
      " - 37s - loss: 0.2857 - acc: 0.8803 - val_loss: 0.3380 - val_acc: 0.8531\n",
      "Epoch 47/50\n",
      " - 36s - loss: 0.2863 - acc: 0.8803 - val_loss: 0.3374 - val_acc: 0.8532\n",
      "Epoch 48/50\n",
      " - 36s - loss: 0.2848 - acc: 0.8807 - val_loss: 0.3329 - val_acc: 0.8569\n",
      "Epoch 49/50\n",
      " - 36s - loss: 0.2848 - acc: 0.8810 - val_loss: 0.3447 - val_acc: 0.8533\n",
      "Epoch 50/50\n",
      " - 36s - loss: 0.2839 - acc: 0.8813 - val_loss: 0.3414 - val_acc: 0.8538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ce6911978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(elmo_train_scaled, y_train, epochs=50, batch_size=32, verbose=2,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267164/267164 [==============================] - 10s 38us/step\n",
      "Training Accuracy : 0.8260544085280951\n"
     ]
    }
   ],
   "source": [
    "dum_pred = model.predict(elmo_train_scaled,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "train_prediction = list(map(get_lable,dum_pred))\n",
    "y_train_lable = list(map(get_lable,y_train))\n",
    "difference = list(map(compare,train_prediction,y_train_lable))\n",
    "\n",
    "print(\"Training Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66791/66791 [==============================] - 2s 37us/step\n",
      "Test Accuracy : 0.7984608704765612\n"
     ]
    }
   ],
   "source": [
    "dum_pred = model.predict(elmo_test_scaled,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "test_prediction = list(map(get_lable,dum_pred))\n",
    "y_test_lable = list(map(get_lable,y_test))\n",
    "difference = list(map(compare,test_prediction,y_test_lable))\n",
    "\n",
    "print(\"Test Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for negatives:0.7803052396228249\n",
      "Precision for positives:0.8019934963947406\n",
      "Precision for neutrals:0.8085497801752362\n",
      "Recall for negatives :0.8329355608591885\n",
      "Recall for positives :0.7116868452418292\n",
      "Recall for neutrals :0.8212249809982265\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_test_lable, name='Actual')\n",
    "y_pred = pd.Series(test_prediction, name='Predicted')\n",
    "df = pd.crosstab(y_actu, y_pred)\n",
    "get_precision(df)\n",
    "print(\"Recall for negatives :\" + str(df.iloc[0][0]/sum(df.iloc[0])))\n",
    "print(\"Recall for positives :\" + str(df.iloc[1][1]/sum(df.iloc[1])))\n",
    "print(\"Recall for neutrals :\" + str(df.iloc[2][2]/sum(df.iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment  confidence_score  \\\n",
      "0         3            0.8129   \n",
      "1         3            1.0000   \n",
      "2         5            0.8468   \n",
      "3         3            0.7997   \n",
      "4         1            1.0000   \n",
      "\n",
      "                                                text  \\\n",
      "0  RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...   \n",
      "1  My cat only chews @apple cords. Such an #Apple...   \n",
      "2  Top 3 all @Apple #tablets. Damn right! http://...   \n",
      "3  Apple Inc. Flash Crash: What You Need to Know ...   \n",
      "4  WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [jpdesloges, aapl, stock, mini, flash, crash, ...   \n",
      "1                [cat, chew, apple, cord, applesnob]   \n",
      "2  [top, apple, tablet, damn, right, http, rjignj...   \n",
      "3  [apple, inc, flash, crash, need, know, http, y...   \n",
      "4       [wtf, battery, one, second, ago, wtf, apple]   \n",
      "\n",
      "                                       clean_comment  \n",
      "0  jpdesloges aapl stock mini flash crash today a...  \n",
      "1                      cat chew apple cord applesnob  \n",
      "2         top apple tablet damn right http rjignjuub  \n",
      "3  apple inc flash crash need know http yjigtifda...  \n",
      "4               wtf battery one second ago wtf apple  \n"
     ]
    }
   ],
   "source": [
    "apple = pd.read_csv(\"apple.csv\")\n",
    "apple = apple[apple.sentiment!=\"not_relevant\"]\n",
    "apple['tokens'] = apple['text'].apply(lambda x : extract_words(x))\n",
    "apple['clean_comment'] = apple['tokens'].apply(lambda x : ' '.join(x))\n",
    "print(apple.head())\n",
    "\n",
    "def apple_lable(x):\n",
    "    if(x=='1'):\n",
    "        a = 0\n",
    "    if(x=='3'):\n",
    "        a = 2\n",
    "    if(x=='5'):\n",
    "        a = 1\n",
    "    return a\n",
    "\n",
    "y_apple= np.array(apple.sentiment)\n",
    "\n",
    "y_apple = list(map(apple_lable,y_apple))\n",
    "\n",
    "y_apple = np_utils.to_categorical(y_apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_dim = 300\n",
    "# apple_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_apple)])\n",
    "# apple_vecs_w2v = scale(apple_vecs_w2v)\n",
    "list_apple = [apple[i:i+1000] for i in range(0,apple.shape[0],1000)]\n",
    "\n",
    "elmo_apple = [None]*len(list_apple)\n",
    "\n",
    "for i in range(0,len(elmo_apple)):\n",
    "    print(i)\n",
    "    x = list_apple[i]\n",
    "    elmo_apple[i] = elmo_vectors(x['clean_comment'])\n",
    "    if(i==len(elmo_apple)):\n",
    "        pickle_out = open(\"elmo_apple.pickle\",\"wb\")\n",
    "        pickle.dump(elmo_apple, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_apple = np.concatenate(elmo_apple,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "elmo_apple_scaled = scale(elmo_apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359/2359 [==============================] - 0s 41us/step\n",
      "Apple Accuracy : 0.7049597286986011\n"
     ]
    }
   ],
   "source": [
    "dum_pred = model.predict(elmo_apple_scaled,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "apple_prediction = list(map(get_lable,dum_pred))\n",
    "y_apple_lable = list(map(get_lable,y_apple))\n",
    "difference = list(map(compare,apple_prediction,y_apple_lable))\n",
    "\n",
    "print(\"Apple Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline = pd.read_csv(\"airline.csv\")\n",
    "airline['tokens'] = airline['text'].apply(lambda x : extract_words(x))\n",
    "airline['clean_comment'] = airline['tokens'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "print(airline.head())\n",
    "\n",
    "def airline_lable(x):\n",
    "    if(x=='negative'):\n",
    "        a = 0\n",
    "    if(x=='neutral'):\n",
    "        a = 2\n",
    "    if(x=='positive'):\n",
    "        a = 1\n",
    "    return a\n",
    "\n",
    "y_airline= np.array(airline.airline_sentiment)\n",
    "y_airline = list(map(airline_lable,y_airline))\n",
    "y_airline = np_utils.to_categorical(y_airline)\n",
    "\n",
    "list_airline = [airline[i:i+1000] for i in range(0,airline.shape[0],1000)]\n",
    "elmo_airline = [None]*len(list_airline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(elmo_airline)):\n",
    "    print(i)\n",
    "    x = list_airline[i]\n",
    "    elmo_airline[i] = elmo_vectors(x['clean_comment'])\n",
    "    if(i==(len(elmo_airline)-1)):\n",
    "        pickle_out = open(\"elmo_airline.pickle\",\"wb\")\n",
    "        pickle.dump(elmo_airline, pickle_out)\n",
    "        pickle_out.close()\n",
    "elmo_airline = np.concatenate(elmo_airline,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2816/10768 [======>.......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/root/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10768/10768 [==============================] - 0s 36us/step\n",
      "Airline Accuracy : 0.6053120356612184\n"
     ]
    }
   ],
   "source": [
    "elmo_airline_scaled = scale(elmo_airline)\n",
    "dum_pred = model.predict(elmo_airline_scaled,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "airline_prediction = list(map(get_lable,dum_pred))\n",
    "y_airline_lable = list(map(get_lable,y_airline))\n",
    "difference = list(map(compare,airline_prediction,y_airline_lable))\n",
    "\n",
    "print(\"Airline Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment  confidence_score  \\\n",
      "0         5            0.7579   \n",
      "1         5            0.8775   \n",
      "3         3            1.0000   \n",
      "4         3            1.0000   \n",
      "5         5            0.7187   \n",
      "\n",
      "                                                text  \\\n",
      "0  Two places I'd invest all my money if I could:...   \n",
      "1  Awesome! Google driverless cars will help the ...   \n",
      "3  Just saw Google self-driving car on I-34. It w...   \n",
      "4  Will driverless cars eventually replace taxi d...   \n",
      "5  Autonomous vehicles could reduce traffic fatal...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [two, place, invest, money, could, printing, s...   \n",
      "1  [awesome, google, driverless, car, help, blind...   \n",
      "3  [saw, google, self, driving, car, painted, gre...   \n",
      "4  [driverless, car, eventually, replace, taxi, d...   \n",
      "5  [autonomous, vehicle, could, reduce, traffic, ...   \n",
      "\n",
      "                                       clean_comment  \n",
      "0  two place invest money could printing self dri...  \n",
      "1  awesome google driverless car help blind trave...  \n",
      "3     saw google self driving car painted green blue  \n",
      "4  driverless car eventually replace taxi driver ...  \n",
      "5   autonomous vehicle could reduce traffic fatality  \n"
     ]
    }
   ],
   "source": [
    "#self drive cars dataset\n",
    "selfdrive = pd.read_csv(\"selfdrive.csv\")\n",
    "selfdrive = selfdrive[~selfdrive.sentiment.isin(['not_relevant','2','4'])]\n",
    "selfdrive['tokens'] = selfdrive['text'].apply(lambda x : extract_words(x))\n",
    "selfdrive['clean_comment'] = selfdrive['tokens'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "print(selfdrive.head())\n",
    "\n",
    "def selfdrive_lable(x):\n",
    "    if(x=='1'):\n",
    "        a = 0\n",
    "    if(x=='3'):\n",
    "        a = 2\n",
    "    if(x=='5'):\n",
    "        a = 1\n",
    "    return a\n",
    "\n",
    "y_selfdrive= np.array(selfdrive.sentiment)\n",
    "y_selfdrive = list(map(selfdrive_lable,y_selfdrive))\n",
    "y_selfdrive = np_utils.to_categorical(y_selfdrive)\n",
    "\n",
    "list_selfdrive = [selfdrive[i:i+1000] for i in range(0,selfdrive.shape[0],1000)]\n",
    "elmo_selfdrive = [None]*len(list_selfdrive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(elmo_selfdrive)):\n",
    "    print(i)\n",
    "    x = list_selfdrive[i]\n",
    "    elmo_selfdrive[i] = elmo_vectors(x['clean_comment'])\n",
    "    if(i==(len(elmo_selfdrive)-1)):\n",
    "        pickle_out = open(\"elmo_selfdrive.pickle\",\"wb\")\n",
    "        pickle.dump(elmo_selfdrive, pickle_out)\n",
    "        pickle_out.close()\n",
    "elmo_selfdrive = np.concatenate(elmo_selfdrive,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3727/3727 [==============================] - 0s 37us/step\n",
      "selfdrive Accuracy : 0.06412664341293266\n"
     ]
    }
   ],
   "source": [
    "#elmo_selfdrive_scaled= scale(elmo_selfdrive)\n",
    "dum_pred = model.predict(elmo_selfdrive_scaled,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "selfdrive_prediction = list(map(get_lable,dum_pred))\n",
    "y_selfdrive_lable = list(map(get_lable,y_selfdrive))\n",
    "difference = list(map(compare,selfdrive_prediction,y_selfdrive_lable))\n",
    "\n",
    "print(\"selfdrive Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40824829,  0.        ,  0.40824829,  0.81649658]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.normalize([[-1,0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfnew = pd.read_csv(\"FB_COMMENTS__Surf_Excel_India_Mar_11___10_59_PM.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "surfnew['tokens'] = surfnew['Comment Text'].apply(lambda x : extract_words(x))\n",
    "surfnew['clean_comment'] = surfnew['tokens'].apply(lambda x : ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_tokens(x):\n",
    "    if(len(x) == 0 ):\n",
    "        return(float('NaN'))\n",
    "    else:\n",
    "        return(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfnew['check_tokens'] = surfnew['tokens'].apply(lambda x : remove_empty_tokens(x) )\n",
    "\n",
    "surfnew =surfnew[pd.notnull(surfnew['check_tokens'])]\n",
    "\n",
    "list_surf = [surfnew[i:i+1000] for i in range(0,surfnew.shape[0],1000)]\n",
    "\n",
    "elmo_surf = [None]*len(list_surf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "for i in range(41,len(elmo_surf)):\n",
    "    print(i)\n",
    "    x = list_surf[i]\n",
    "    elmo_surf[i] = elmo_vectors(x['clean_comment'])\n",
    "    if(i==(len(elmo_surf)-1)):\n",
    "        pickle_out = open(\"elmo_surf.pickle\",\"wb\")\n",
    "        pickle.dump(elmo_surf, pickle_out)\n",
    "        pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"elmo_surf.pickle\", \"rb\")\n",
    "elmo_surf = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open(\"elmo_surf.pickle\",\"wb\")\n",
    "# pickle.dump(elmo_surf, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_surf2 = np.concatenate(elmo_surf,axis = 0)\n",
    "elmo_surf_scaled = scale(elmo_surf2)\n",
    "\n",
    "dum_pred = model.predict(elmo_surf_scaled,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "surfnew_prediction = list(map(get_lable,dum_pred))\n",
    "\n",
    "pd.DataFrame(surfnew_prediction).to_csv(\"elmo_surf_pred.csv\",index = False)\n",
    "\n",
    "surfnew['elmo_results'] = surfnew_prediction\n",
    "\n",
    "surfnew.to_csv(\"elmo_surf_pred.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
