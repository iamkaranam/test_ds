{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers import Activation\n",
    "from scipy.special import softmax\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon_lable(x):\n",
    "    a=-1\n",
    "    if(x=='POSITIVE'):\n",
    "        a = 1\n",
    "    if(x=='NEGATIVE'):\n",
    "        a = 0\n",
    "    if(x=='NEUTRAL'):\n",
    "        a = 2\n",
    "    return a\n",
    "\n",
    "def bing_lable(dum):\n",
    "    a=-1\n",
    "    if(dum>= 0 and dum<=0.3):\n",
    "        a = 0\n",
    "    if(dum>0.3 and dum<0.7):\n",
    "        a = 2\n",
    "    if(dum>=0.7 and dum<=1):\n",
    "        a = 1\n",
    "    return a    \n",
    "\n",
    "def google_lable(x):\n",
    "    a=-1\n",
    "    if(x > -0.3 and x < 0.3):\n",
    "        a = 2\n",
    "    if( x >= 0.3 ):\n",
    "        a = 1\n",
    "    if(x <= -0.3):\n",
    "        a = 0\n",
    "    return a\n",
    "\n",
    "def compare_apis(w,x,y):\n",
    "    if(w==x==y):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "\n",
    "def unmetric_lable(x):\n",
    "    if(x=='+'):\n",
    "        a = 1\n",
    "    if(x=='-'):\n",
    "        a = 0\n",
    "    if((x==0) or (x=='0')):\n",
    "        a = 2\n",
    "    if(x=='U'):\n",
    "        a = 3\n",
    "    return a\n",
    "\n",
    "def get_scores(x):\n",
    "    xsum = sum(x)\n",
    "    xneg = round(x[0]/xsum,2)\n",
    "    xpos = round(x[1]/xsum,2)\n",
    "    xneut = round(x[2]/xsum,2)\n",
    "    return([xneg,xpos,xneut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = pd.read_csv(\"stopwords2.csv\")\n",
    "custom_stopwords = list(custom_stopwords['StopWords'])\n",
    "\n",
    "def extract_words(temp):\n",
    "    #temp = re.sub(r'[^\\w\\s]','',temp) #punctuation\n",
    "    temp = re.sub(r'http\\S+', '', temp) #remove links\n",
    "    temp = \" \".join(list(filter(lambda x:x[0]!='@', temp.split())))\n",
    "    temp = re.sub(r\"[^a-zA-Z0-9'\\s]\", ' ', temp)\n",
    "    temp = temp.lower()\n",
    "    temp = temp.split()\n",
    "#     temp = tb(temp)        \n",
    "#     lista = [word for word in temp.words if word not in stopwords.words('english')]\n",
    "    lista = [word for word in temp if word not in custom_stopwords]\n",
    "    listb = [lemmatizer.lemmatize(y) for y in lista]\n",
    "    #listb = [stemmer.stem(y) for y in listb]\n",
    "    listb = [word for word in listb if len(word)>2]\n",
    "    return(listb)\n",
    "        \n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    i = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += glove_w2vec[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.  * tfidf[word]\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def get_lable(b):\n",
    "    a = list(b)\n",
    "    i = a.index(max(a))\n",
    "    if(i==0):\n",
    "        lable = 0\n",
    "    if(i==1):\n",
    "        lable = 1\n",
    "    if(i==2):\n",
    "        lable = 2\n",
    "    return lable\n",
    "\n",
    "def compare(x1,x2):\n",
    "    if(x1==x2):\n",
    "        c = 0\n",
    "    else:\n",
    "        c = 1\n",
    "    return c\n",
    "\n",
    "def get_precision(x):\n",
    "    df = x\n",
    "    print(\"Precision for negatives:\" +str(df.iloc[0][0]/sum(df[0])))\n",
    "    print(\"Precision for positives:\" + str(df.iloc[1][1]/sum(df[1])))\n",
    "    print(\"Precision for neutrals:\" + str(df.iloc[2][2]/sum(df[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pymysql.connect(host='ec2-75-101-133-61.compute-1.amazonaws.com',\n",
    "    \t\t\t\t\t\t\t user='umtdev',\n",
    "    \t\t\t\t\t\t\t password='GX^2M$4%781!ra1t-e~',\n",
    "    \t\t\t\t\t\t\t db='unmetric',\n",
    "    \t\t\t\t\t\t\t charset='utf8mb4',\n",
    "    \t\t\t\t\t\t\t cursorclass=pymysql.cursors.DictCursor,autocommit = True)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "query = str(\"select a.fbcomment_id,a.text,b.sentiment from fb_comment_text a join\\\n",
    "            fb_sentiment_comments_comprehend b on b.fbpagepostcomments_id = a.fbcomment_id;\")\n",
    "# query = str(\"select a.fbcomment_id,a.text,b.sentiment as amazon_score ,c.sentiment_score as bing_score,\\\n",
    "#             d.score as google_score ,e.sentiment_score as unmetric_score,e.sentiment as unmetric_sentiment from fb_comment_text a join\\\n",
    "#             fb_sentiment_comments_comprehend b on b.fbpagepostcomments_id = a.fbcomment_id\\\n",
    "#             join fb_sentiment_comments_bing c on c.fbpagepostcomments_id = a.fbcomment_id\\\n",
    "#             join fb_sentiment_comments_google_nlp d on d.fbpagepostcomments_id = a.fbcomment_id\\\n",
    "#              join fb_pagepost_comments e on e.id = a.fbcomment_id;\")\n",
    "request = cursor.execute(query)\n",
    "comments_amazon = pd.DataFrame(cursor.fetchall())\n",
    "# all_posts = pd.DataFrame(cursor.fetchall())\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333955\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fbcomment_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>senti_lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9065432149</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>We would upgrade to the A380 anyway! Haha</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8841832192</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>\"They understand this and have no problem reso...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fbcomment_id sentiment                                               text  \\\n",
       "0    9065432149   NEUTRAL          We would upgrade to the A380 anyway! Haha   \n",
       "1    8841832192  NEGATIVE  \"They understand this and have no problem reso...   \n",
       "\n",
       "   senti_lable  \n",
       "0            2  \n",
       "1            0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_amazon['senti_lable'] = comments_amazon['sentiment'].apply(lambda x : amazon_lable(x))\n",
    "# print(len(all_posts))\n",
    "# all_posts['amazon_lable'] = all_posts['amazon_score'].apply(lambda x : amazon_lable(x))\n",
    "# all_posts['bing_lable'] = all_posts['bing_score'].apply(lambda x : bing_lable(x))\n",
    "# all_posts['google_lable'] = all_posts['google_score'].apply(lambda x : google_lable(x))\n",
    "# all_posts['common'] = list(map(compare_apis,all_posts['amazon_lable'],all_posts['bing_lable'],all_posts['google_lable']))\n",
    "# comments_amazon = all_posts[all_posts.common==1]\n",
    "\n",
    "print(len(comments_amazon))\n",
    "comments_amazon.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pooling...\n",
      "Pooling Finished...\n",
      "   fbcomment_id sentiment                                               text  \\\n",
      "0    9065432149   NEUTRAL          We would upgrade to the A380 anyway! Haha   \n",
      "1    8841832192  NEGATIVE  \"They understand this and have no problem reso...   \n",
      "2    8841832194  NEGATIVE  #topthreenotsocustomerservicemoments After bum...   \n",
      "3    8841832196   NEUTRAL          Hey Laurie, can you send both please? ^EH   \n",
      "4    8967296634  NEGATIVE  I am so incredibly disappointed with my servic...   \n",
      "\n",
      "   senti_lable                                             tokens  \n",
      "0            2               [would, upgrade, a380, anyway, haha]  \n",
      "1            0  [understand, problem, resolving, baggage, issu...  \n",
      "2            0  [topthreenotsocustomerservicemoments, bumping,...  \n",
      "3            2                   [hey, laurie, can, send, please]  \n",
      "4            0  [incredibly, disappointed, service, united, fa...  \n",
      "333955\n"
     ]
    }
   ],
   "source": [
    "pool = Pool(processes=cpu_count()-1) \n",
    "print(\"Starting Pooling...\")\n",
    "results =  pool.map(extract_words, comments_amazon['text'])\n",
    "print(\"Pooling Finished...\")\n",
    "pool.close()\n",
    "pool.join()\n",
    "comments_amazon['tokens'] = results\n",
    "print(comments_amazon.head())\n",
    "print(len(comments_amazon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 157141, 0: 96152, 1: 79778, -1: 884})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(comments_amazon['senti_lable']))\n",
    "# comments_amazon['text'] = comments_amazon['text'].apply(lambda x : x.replace('\\n',''))\n",
    "# comments_amazon['text'] = comments_amazon['text'].apply(lambda x : x.replace('\\r',''))\n",
    "# comments_amazon.to_csv(\"amazon_text.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333071\n",
      "Counter({2: 157141, 0: 96152, 1: 79778})\n"
     ]
    }
   ],
   "source": [
    "comments_amazon = comments_amazon[comments_amazon.senti_lable!=-1]\n",
    "print(len(comments_amazon))\n",
    "print(Counter(comments_amazon['senti_lable']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fbcomment_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>senti_lable</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9065432149</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>We would upgrade to the A380 anyway! Haha</td>\n",
       "      <td>2</td>\n",
       "      <td>[would, upgrade, a380, anyway, haha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8841832192</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>\"They understand this and have no problem reso...</td>\n",
       "      <td>0</td>\n",
       "      <td>[understand, no, problem, resolving, baggage, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8841832194</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>#topthreenotsocustomerservicemoments After bum...</td>\n",
       "      <td>0</td>\n",
       "      <td>[topthreenotsocustomerservicemoments, bumping,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8841832196</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Hey Laurie, can you send both please? ^EH</td>\n",
       "      <td>2</td>\n",
       "      <td>[hey, laurie, can, send, please, eh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8967296634</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>I am so incredibly disappointed with my servic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[incredibly, disappointed, service, united, fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fbcomment_id sentiment                                               text  \\\n",
       "0    9065432149   NEUTRAL          We would upgrade to the A380 anyway! Haha   \n",
       "1    8841832192  NEGATIVE  \"They understand this and have no problem reso...   \n",
       "2    8841832194  NEGATIVE  #topthreenotsocustomerservicemoments After bum...   \n",
       "3    8841832196   NEUTRAL          Hey Laurie, can you send both please? ^EH   \n",
       "4    8967296634  NEGATIVE  I am so incredibly disappointed with my servic...   \n",
       "\n",
       "   senti_lable                                             tokens  \n",
       "0            2               [would, upgrade, a380, anyway, haha]  \n",
       "1            0  [understand, no, problem, resolving, baggage, ...  \n",
       "2            0  [topthreenotsocustomerservicemoments, bumping,...  \n",
       "3            2               [hey, laurie, can, send, please, eh]  \n",
       "4            0  [incredibly, disappointed, service, united, fa...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_amazon = comments_amazon.rename(columns = {'amazon_lable':'senti_lable'})\n",
    "comments_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comments_amazon[{'text','tokens','senti_lable','fbcomment_id'}]\n",
    "#,'fbcomment_id'\n",
    "# data = data.sample(frac = 1)\n",
    "\n",
    "# x_train, x_test, y_train, y_test,z_train,z_test= train_test_split(np.array(data.tokens),\n",
    "#                                                             np.array(data.senti_lable),np.array(data.fbcomment_id), test_size=0.2)\n",
    "\n",
    "# pd.DataFrame(z_train).to_csv(\"/external_ebs/SentimentAnalysis_FB/train_ids.csv\")\n",
    "# pd.DataFrame(z_test).to_csv(\"/external_ebs/SentimentAnalysis_FB/test_ids.csv\")\n",
    "\n",
    "# y_train = np_utils.to_categorical(y_train)\n",
    "# y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti_lable</th>\n",
       "      <th>tokens</th>\n",
       "      <th>text</th>\n",
       "      <th>fbcomment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>[would, upgrade, a380, anyway, haha]</td>\n",
       "      <td>We would upgrade to the A380 anyway! Haha</td>\n",
       "      <td>9065432149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[understand, problem, resolving, baggage, issu...</td>\n",
       "      <td>\"They understand this and have no problem reso...</td>\n",
       "      <td>8841832192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[topthreenotsocustomerservicemoments, bumping,...</td>\n",
       "      <td>#topthreenotsocustomerservicemoments After bum...</td>\n",
       "      <td>8841832194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[hey, laurie, can, send, please]</td>\n",
       "      <td>Hey Laurie, can you send both please? ^EH</td>\n",
       "      <td>8841832196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[incredibly, disappointed, service, united, fa...</td>\n",
       "      <td>I am so incredibly disappointed with my servic...</td>\n",
       "      <td>8967296634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   senti_lable                                             tokens  \\\n",
       "0            2               [would, upgrade, a380, anyway, haha]   \n",
       "1            0  [understand, problem, resolving, baggage, issu...   \n",
       "2            0  [topthreenotsocustomerservicemoments, bumping,...   \n",
       "3            2                   [hey, laurie, can, send, please]   \n",
       "4            0  [incredibly, disappointed, service, united, fa...   \n",
       "\n",
       "                                                text  fbcomment_id  \n",
       "0          We would upgrade to the A380 anyway! Haha    9065432149  \n",
       "1  \"They understand this and have no problem reso...    8841832192  \n",
       "2  #topthreenotsocustomerservicemoments After bum...    8841832194  \n",
       "3          Hey Laurie, can you send both please? ^EH    8841832196  \n",
       "4  I am so incredibly disappointed with my servic...    8967296634  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.840B.300d.txt'\n",
    "word2vec_output_file = 'glove.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "from gensim.models import KeyedVectors\n",
    "glove_w2vec = KeyedVectors.load_word2vec_format('glove.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 125738, 0: 76878, 1: 63837})\n"
     ]
    }
   ],
   "source": [
    "ids_train = pd.read_csv(\"amazon_train_ids.csv\")\n",
    "\n",
    "ids_train = np.array(ids_train['0'])\n",
    "\n",
    "data_train = data[data.fbcomment_id.isin(ids_train)]\n",
    "\n",
    "print(Counter(data_train['senti_lable']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = data\n",
    "# data_train = data_train.sample(frac = 0.8)\n",
    "x_train = np.array(data_train.tokens)\n",
    "\n",
    "y_train = np.array(data_train.senti_lable)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# ids_test = pd.read_csv(\"amazon_test_ids.csv\")\n",
    "\n",
    "# ids_test = np.array(ids_test['0'])\n",
    "\n",
    "# data_test = data[data.fbcomment_id.isin(ids_test)]\n",
    "\n",
    "# x_test = np.array(data_test.tokens)\n",
    "\n",
    "# y_test = np.array(data_test.senti_lable)\n",
    "# y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google_w2vec = gensim.models.KeyedVectors.load_word2vec_format(r'/external_ebs/SentimentAnalysis_FB/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 300\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_train)])\n",
    "#train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "# n_dim = 300\n",
    "# test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_test)])\n",
    "#test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266453, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='elu', input_dim=300))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "#model.fit(elmo_train_scaled, y_train, epochs=15, batch_size=32, verbose=2,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 213162 samples, validate on 53291 samples\n",
      "Epoch 1/50\n",
      " - 38s - loss: 0.3054 - acc: 0.8740 - val_loss: 0.3220 - val_acc: 0.8599\n",
      "Epoch 2/50\n",
      " - 32s - loss: 0.3041 - acc: 0.8743 - val_loss: 0.3245 - val_acc: 0.8607\n",
      "Epoch 3/50\n",
      " - 31s - loss: 0.3035 - acc: 0.8746 - val_loss: 0.3272 - val_acc: 0.8593\n",
      "Epoch 4/50\n",
      " - 31s - loss: 0.3040 - acc: 0.8744 - val_loss: 0.3269 - val_acc: 0.8577\n",
      "Epoch 5/50\n",
      " - 31s - loss: 0.3045 - acc: 0.8742 - val_loss: 0.3247 - val_acc: 0.8593\n",
      "Epoch 6/50\n",
      " - 30s - loss: 0.3033 - acc: 0.8748 - val_loss: 0.3264 - val_acc: 0.8596\n",
      "Epoch 7/50\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_vecs_w2v, y_train, epochs=50, batch_size=32, verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save a model\n",
    "# model_glove7_amazon_json = model.to_json()\n",
    "\n",
    "# with open(\"model_glove7_amazon.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_glove7_amazon_json)\n",
    "\n",
    "# model.save_weights(\"model_glove7_amazon.h5\")\n",
    "model.save(\"model_glove_amazon.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_glove7_amazon = load_model('model_glove7_amazon.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load saved model \n",
    "json_file = open('model_glove4_amazon.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_glove4_amazon = model_from_json(loaded_model_json)\n",
    "model_glove4_amazon.load_weights(\"model_glove4_amazon.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266453/266453 [==============================] - 10s 37us/step\n",
      "Training Accuracy : 0.8152544726462078\n"
     ]
    }
   ],
   "source": [
    "#for training error\n",
    "dum_pred = model.predict(train_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "train_prediction = list(map(get_lable,dum_pred))\n",
    "#train_prediction=data_train['text'].apply(lambda x : make_predictions(x))\n",
    "# train_text = data_train['text']\n",
    "\n",
    "# for i in range(len(train_text)):\n",
    "#     if(i%10000==0):\n",
    "#         print(i)\n",
    "#     xtext = train_text.iloc[i]\n",
    "#     train_prediction.append(make_predictions(xtext))\n",
    "\n",
    "# train_pred = train_prediction\n",
    "\n",
    "# train_prediction = sum(train_pred,[])\n",
    "y_train_lable = list(map(get_lable,y_train))\n",
    "difference = list(map(compare,train_prediction,y_train_lable))\n",
    "\n",
    "print(\"Training Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actu = pd.Series(y_train_lable, name='Actual')\n",
    "y_pred = pd.Series(train_prediction, name='Predicted')\n",
    "df = pd.crosstab(y_actu, y_pred)\n",
    "get_precision(df)\n",
    "\n",
    "print(\"Recall for negatives :\" + str(df.iloc[0][0]/sum(df.iloc[0])))\n",
    "print(\"Recall for positives :\" + str(df.iloc[1][1]/sum(df.iloc[1])))\n",
    "print(\"Recall for neutrals :\" + str(df.iloc[2][2]/sum(df.iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66791/66791 [==============================] - 3s 38us/step\n",
      "Test Accuracy : 0.8028776332140558\n"
     ]
    }
   ],
   "source": [
    "#for test error\n",
    "dum_pred = model.predict(test_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "test_prediction = list(map(get_lable,dum_pred))\n",
    "\n",
    "# test_prediction=data_test['text'].progress_apply(lambda x : make_predictions(x))\n",
    "y_test_lable = list(map(get_lable,y_test))\n",
    "difference = list(map(compare,test_prediction,y_test_lable))\n",
    "print(\"Test Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actu = pd.Series(y_test_lable, name='Actual')\n",
    "y_pred = pd.Series(test_prediction, name='Predicted')\n",
    "df = pd.crosstab(y_actu, y_pred)\n",
    "get_precision(df)\n",
    "print(\"Recall for negatives :\" + str(df.iloc[0][0]/sum(df.iloc[0])))\n",
    "print(\"Recall for positives :\" + str(df.iloc[1][1]/sum(df.iloc[1])))\n",
    "print(\"Recall for neutrals :\" + str(df.iloc[2][2]/sum(df.iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment  confidence_score  \\\n",
      "0         5            0.7579   \n",
      "1         5            0.8775   \n",
      "3         3            1.0000   \n",
      "4         3            1.0000   \n",
      "5         5            0.7187   \n",
      "\n",
      "                                                text  \\\n",
      "0  Two places I'd invest all my money if I could:...   \n",
      "1  Awesome! Google driverless cars will help the ...   \n",
      "3  Just saw Google self-driving car on I-34. It w...   \n",
      "4  Will driverless cars eventually replace taxi d...   \n",
      "5  Autonomous vehicles could reduce traffic fatal...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [two, place, invest, money, could, 3d, printin...  \n",
      "1  [awesome, google, driverless, car, help, blind...  \n",
      "3  [saw, google, self, driving, car, 34, painted,...  \n",
      "4  [driverless, car, eventually, replace, taxi, d...  \n",
      "5  [autonomous, vehicle, could, reduce, traffic, ...  \n",
      "3727/3727 [==============================] - 0s 37us/step\n",
      "selfdrive Accuracy : 0.9173598068151328\n"
     ]
    }
   ],
   "source": [
    "#self drive cars dataset\n",
    "selfdrive = pd.read_csv(\"selfdrive.csv\")\n",
    "selfdrive = selfdrive[~selfdrive.sentiment.isin(['not_relevant','2','4'])]\n",
    "selfdrive['tokens'] = selfdrive['text'].apply(lambda x : extract_words(x))\n",
    "print(selfdrive.head())\n",
    "\n",
    "def selfdrive_lable(x):\n",
    "    if(x=='1'):\n",
    "        a = 0\n",
    "    if(x=='3'):\n",
    "        a = 2\n",
    "    if(x=='5'):\n",
    "        a = 1\n",
    "    return a\n",
    "\n",
    "y_selfdrive= np.array(selfdrive.sentiment)\n",
    "y_selfdrive = list(map(selfdrive_lable,y_selfdrive))\n",
    "y_selfdrive = np_utils.to_categorical(y_selfdrive)\n",
    "\n",
    "# selfdrive_prediction = pd.read_csv(\"selfdrive_stanford.csv\",header = None)\n",
    "\n",
    "# selfdrive_prediction.head()\n",
    "\n",
    "# selfdrive_prediction.columns= [\"result\"]\n",
    "\n",
    "# selfdrive_prediction = list(selfdrive_prediction['result'])\n",
    "x_selfdrive = np.array(selfdrive.tokens)\n",
    "n_dim = 300\n",
    "selfdrive_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_selfdrive)])\n",
    "selfdrive_vecs_w2v = scale(selfdrive_vecs_w2v)\n",
    "\n",
    "dum_pred = model.predict(selfdrive_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "selfdrive_prediction = list(map(get_lable,dum_pred))\n",
    "y_selfdrive_lable = list(map(get_lable,y_selfdrive))\n",
    "difference = list(map(compare,selfdrive_prediction,y_selfdrive_lable))\n",
    "\n",
    "print(\"selfdrive Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for negatives:0.04918032786885246\n",
      "Precision for positives:0.3141025641025641\n",
      "Precision for neutrals:0.9528920691711389\n",
      "Recall for negatives :0.05660377358490566\n",
      "Recall for positives :0.4298245614035088\n",
      "Recall for neutrals :0.9274521183981428\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_selfdrive_lable, name='Actual')\n",
    "y_pred = pd.Series(selfdrive_prediction, name='Predicted')\n",
    "df = pd.crosstab(y_actu, y_pred)\n",
    "get_precision(df)\n",
    "\n",
    "print(\"Recall for negatives :\" + str(df.iloc[0][0]/sum(df.iloc[0])))\n",
    "print(\"Recall for positives :\" + str(df.iloc[1][1]/sum(df.iloc[1])))\n",
    "print(\"Recall for neutrals :\" + str(df.iloc[2][2]/sum(df.iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment  confidence_score  \\\n",
      "0         3            0.8129   \n",
      "1         3            1.0000   \n",
      "2         5            0.8468   \n",
      "3         3            0.7997   \n",
      "4         1            1.0000   \n",
      "\n",
      "                                                text  \\\n",
      "0  RT @JPDesloges: Why AAPL Stock Had a Mini-Flas...   \n",
      "1  My cat only chews @apple cords. Such an #Apple...   \n",
      "2  Top 3 all @Apple #tablets. Damn right! http://...   \n",
      "3  Apple Inc. Flash Crash: What You Need to Know ...   \n",
      "4  WTF MY BATTERY WAS 31% ONE SECOND AGO AND NOW ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [rt, why, aapl, stock, mini, flash, crash, tod...  \n",
      "1                       [cat, chew, cord, applesnob]  \n",
      "2                    [top, all, tablet, damn, right]  \n",
      "3       [apple, inc, flash, crash, need, know, aapl]  \n",
      "4      [wtf, battery, 31, one, second, ago, 29, wtf]  \n",
      "2359/2359 [==============================] - 0s 39us/step\n",
      "Apple Accuracy : 0.7918609580330649\n"
     ]
    }
   ],
   "source": [
    "apple = pd.read_csv(\"apple.csv\")\n",
    "apple = apple[apple.sentiment!=\"not_relevant\"]\n",
    "apple['tokens'] = apple['text'].apply(lambda x : extract_words(x))\n",
    "print(apple.head())\n",
    "\n",
    "def apple_lable(x):\n",
    "    if(x=='1'):\n",
    "        a = 0\n",
    "    if(x=='3'):\n",
    "        a = 2\n",
    "    if(x=='5'):\n",
    "        a = 1\n",
    "    return a\n",
    "\n",
    "y_apple= np.array(apple.sentiment)\n",
    "\n",
    "y_apple = list(map(apple_lable,y_apple))\n",
    "\n",
    "y_apple = np_utils.to_categorical(y_apple)\n",
    "\n",
    "# apple_prediction = pd.read_csv(\"apple_stanford.csv\",header = None)\n",
    "\n",
    "# apple_prediction.head()\n",
    "\n",
    "# apple_prediction.columns= [\"result\"]\n",
    "\n",
    "# apple_prediction = list(apple_prediction['result'])\n",
    "\n",
    "\n",
    "x_apple = np.array(apple.tokens)\n",
    "\n",
    "n_dim = 300\n",
    "apple_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_apple)])\n",
    "#apple_vecs_w2v = scale(apple_vecs_w2v)\n",
    "\n",
    "dum_pred = model.predict(apple_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "apple_prediction = list(map(get_lable,dum_pred))\n",
    "\n",
    "y_apple_lable = list(map(get_lable,y_apple))\n",
    "difference = list(map(compare,apple_prediction,y_apple_lable))\n",
    "\n",
    "print(\"Apple Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for negatives:0.9214145383104125\n",
      "Precision for positives:0.5364806866952789\n",
      "Precision for neutrals:0.764378478664193\n",
      "Recall for negatives :0.5637019230769231\n",
      "Recall for positives :0.5952380952380952\n",
      "Recall for neutrals :0.9384965831435079\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_apple_lable, name='Actual')\n",
    "y_pred = pd.Series(apple_prediction, name='Predicted')\n",
    "df = pd.crosstab(y_actu, y_pred)\n",
    "get_precision(df)\n",
    "\n",
    "print(\"Recall for negatives :\" + str(df.iloc[0][0]/sum(df.iloc[0])))\n",
    "print(\"Recall for positives :\" + str(df.iloc[1][1]/sum(df.iloc[1])))\n",
    "print(\"Recall for neutrals :\" + str(df.iloc[2][2]/sum(df.iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  airline_sentiment  confidence_score  \\\n",
      "0           neutral               1.0   \n",
      "1          negative               1.0   \n",
      "2          negative               1.0   \n",
      "3          negative               1.0   \n",
      "4          positive               1.0   \n",
      "\n",
      "                                                text  \\\n",
      "0                @VirginAmerica What @dhepburn said.   \n",
      "1  @VirginAmerica it's really aggressive to blast...   \n",
      "2  @VirginAmerica and it's a really big bad thing...   \n",
      "3  @VirginAmerica seriously would pay $30 a fligh...   \n",
      "4  @VirginAmerica it was amazing, and arrived an ...   \n",
      "\n",
      "                                              tokens  \n",
      "0                                             [said]  \n",
      "1  [really, aggressive, blast, obnoxious, enterta...  \n",
      "2                          [really, big, bad, thing]  \n",
      "3  [seriously, would, pay, 30, flight, seat, didn...  \n",
      "4         [amazing, arrived, hour, early, too, good]  \n",
      "10768/10768 [==============================] - 0s 39us/step\n",
      "Airline Accuracy : 0.6418090638930164\n"
     ]
    }
   ],
   "source": [
    "airline = pd.read_csv(\"airline.csv\")\n",
    "airline['tokens'] = airline['text'].apply(lambda x : extract_words(x))\n",
    "print(airline.head())\n",
    "\n",
    "def airline_lable(x):\n",
    "    if(x=='negative'):\n",
    "        a = 0\n",
    "    if(x=='neutral'):\n",
    "        a = 2\n",
    "    if(x=='positive'):\n",
    "        a = 1\n",
    "    return a\n",
    "\n",
    "y_airline= np.array(airline.airline_sentiment)\n",
    "y_airline = list(map(airline_lable,y_airline))\n",
    "y_airline = np_utils.to_categorical(y_airline)\n",
    "\n",
    "# airline_prediction = pd.read_csv(\"airline_stanford.csv\",header = None)\n",
    "\n",
    "# airline_prediction.head()\n",
    "\n",
    "# airline_prediction.columns= [\"result\"]\n",
    "\n",
    "# airline_prediction = list(airline_prediction['result'])\n",
    "x_airline = np.array(airline.tokens)\n",
    "n_dim = 300\n",
    "airline_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_airline)])\n",
    "#airline_vecs_w2v = scale(airline_vecs_w2v)\n",
    "\n",
    "dum_pred = model.predict(airline_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "airline_prediction = list(map(get_lable,dum_pred))\n",
    "y_airline_lable = list(map(get_lable,y_airline))\n",
    "difference = list(map(compare,airline_prediction,y_airline_lable))\n",
    "\n",
    "print(\"Airline Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_airline= np.array(airline.airline_sentiment)\n",
    "y_airline = list(map(airline_lable,y_airline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline['actual_score'] = y_airline_lable\n",
    "\n",
    "airline['new_score'] = airline_prediction\n",
    "\n",
    "airline.to_csv(\"airline_result.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for negatives:0.9656593406593407\n",
      "Precision for positives:0.6571287617168229\n",
      "Precision for neutrals:0.2640658694373652\n",
      "Recall for negatives :0.4654396186440678\n",
      "Recall for positives :0.8419721871049305\n",
      "Recall for neutrals :0.8243574051407588\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_airline_lable, name='Actual')\n",
    "y_pred = pd.Series(airline_prediction, name='Predicted')\n",
    "df = pd.crosstab(y_actu, y_pred)\n",
    "get_precision(df)\n",
    "\n",
    "print(\"Recall for negatives :\" + str(df.iloc[0][0]/sum(df.iloc[0])))\n",
    "print(\"Recall for positives :\" + str(df.iloc[1][1]/sum(df.iloc[1])))\n",
    "print(\"Recall for neutrals :\" + str(df.iloc[2][2]/sum(df.iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text         sentiment  \\\n",
      "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  Negative emotion   \n",
      "1  @jessedee Know about @fludapp ? Awesome iPad/i...  Positive emotion   \n",
      "2  @swonderlin Can not wait for #iPad 2 also. The...  Positive emotion   \n",
      "3  @sxsw I hope this year's festival isn't as cra...  Negative emotion   \n",
      "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  Positive emotion   \n",
      "\n",
      "                                              tokens  \n",
      "0  [wesley83, 3g, iphone, hr, tweeting, rise, aus...  \n",
      "1  [know, awesome, ipad, iphone, app, likely, app...  \n",
      "2   [can, not, wait, ipad, also, should, sale, sxsw]  \n",
      "3  [hope, year, festival, isn, crashy, year, ipho...  \n",
      "4  [great, stuff, fri, sxsw, marissa, mayer, goog...  \n",
      "9092/9092 [==============================] - 0s 38us/step\n",
      "Products Accuracy : 0.6319841619005719\n"
     ]
    }
   ],
   "source": [
    "products = pd.read_csv(\"products.csv\")\n",
    "products['tokens'] = products['text'].apply(lambda x : extract_words(x))\n",
    "print(products.head())\n",
    "\n",
    "def products_lable(x):\n",
    "    if(x==\"Negative emotion\"):\n",
    "        a = 0\n",
    "    if(x==\"I can't tell\" or x == \"No emotion toward brand or product\"):\n",
    "        a = 2\n",
    "    if(x=='Positive emotion'):\n",
    "        a = 1\n",
    "    return a\n",
    "\n",
    "y_products= np.array(products.sentiment)\n",
    "y_products = list(map(products_lable,y_products))\n",
    "y_products = np_utils.to_categorical(y_products)\n",
    "\n",
    "# products_prediction = pd.read_csv(\"products_stanford.csv\",header = None)\n",
    "\n",
    "# products_prediction.columns= [\"result\"]\n",
    "\n",
    "# products_prediction = list(products_prediction['result'])\n",
    "\n",
    "x_products = np.array(products.tokens)\n",
    "n_dim = 300\n",
    "products_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_products)])\n",
    "#products_vecs_w2v = scale(products_vecs_w2v)\n",
    "\n",
    "dum_pred = model.predict(products_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "products_prediction = list(map(get_lable,dum_pred))\n",
    "y_products_lable = list(map(get_lable,y_products))\n",
    "difference = list(map(compare,products_prediction,y_products_lable))\n",
    "\n",
    "print(\"Products Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['actual_score'] = y_products_lable\n",
    "\n",
    "products['new_score'] = products_prediction\n",
    "\n",
    "products.to_csv(\"products_result.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for negatives:0.42574257425742573\n",
      "Precision for positives:0.659\n",
      "Precision for neutrals:0.6514829182830685\n",
      "Recall for negatives :0.07543859649122807\n",
      "Recall for positives :0.22128945601074546\n",
      "Recall for neutrals :0.939033189033189\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_products_lable, name='Actual')\n",
    "y_pred = pd.Series(products_prediction, name='Predicted')\n",
    "df = pd.crosstab(y_actu, y_pred)\n",
    "get_precision(df)\n",
    "\n",
    "print(\"Recall for negatives :\" + str(df.iloc[0][0]/sum(df.iloc[0])))\n",
    "print(\"Recall for positives :\" + str(df.iloc[1][1]/sum(df.iloc[1])))\n",
    "print(\"Recall for neutrals :\" + str(df.iloc[2][2]/sum(df.iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25946\n",
      "25931\n",
      "25931/25931 [==============================] - 1s 36us/step\n",
      "Combined Accuracy : 0.6697774864062319\n"
     ]
    }
   ],
   "source": [
    "combined = pd.read_csv(\"combined_data.csv\")\n",
    "combined['tokens'] = combined['text'].apply(lambda x : extract_words(x))\n",
    "combined['num_words'] = combined['tokens'].apply(lambda x : len(x))\n",
    "print(len(combined))\n",
    "combined = combined[combined.num_words>0]\n",
    "print(len(combined))\n",
    "\n",
    "y_combined= np.array(combined.sentiment_lable)\n",
    "y_combined = np_utils.to_categorical(y_combined)\n",
    "\n",
    "x_combined = np.array(combined.tokens)\n",
    "n_dim = 300\n",
    "combined_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_combined)])\n",
    "#combined_vecs_w2v = scale(combined_vecs_w2v)\n",
    "\n",
    "dum_pred = model.predict(combined_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "\n",
    "combined_prediction = list(map(get_lable,dum_pred))\n",
    "y_combined_lable = list(map(get_lable,y_combined))\n",
    "difference = list(map(compare,combined_prediction,y_combined_lable))\n",
    "\n",
    "print(\"Combined Accuracy : \"+str((len(difference)-sum(difference))/(len(difference))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['actual_score'] = y_combined_lable\n",
    "combined['new_score'] = combined_prediction\n",
    "# combined.to_csv(\"combined_result4.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940\n"
     ]
    }
   ],
   "source": [
    "combined['compare'] = list(map(compare_custom,combined['actual_score'],combined['new_score']))\n",
    "xdel = combined[combined.compare==1]\n",
    "print(len(xdel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for negatives:0.911747401451265\n",
      "Precision for positives:0.6776964683423481\n",
      "Precision for neutrals:0.6241732149923681\n",
      "Recall for negatives :0.5162114146124805\n",
      "Recall for positives :0.42634107285828665\n",
      "Recall for neutrals :0.9255595607343449\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_combined_lable, name='Actual')\n",
    "y_pred = pd.Series(combined_prediction, name='Predicted')\n",
    "df = pd.crosstab(y_actu, y_pred)\n",
    "get_precision(df)\n",
    "\n",
    "print(\"Recall for negatives :\" + str(df.iloc[0][0]/sum(df.iloc[0])))\n",
    "print(\"Recall for positives :\" + str(df.iloc[1][1]/sum(df.iloc[1])))\n",
    "print(\"Recall for neutrals :\" + str(df.iloc[2][2]/sum(df.iloc[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdum_pred = dum_pred.tolist()\n",
    "print(len(xdum_pred))\n",
    "xdum_pred = list(map(get_scores,xdum_pred))\n",
    "print(len(xdum_pred))\n",
    "xdum_pred_df = pd.DataFrame(xdum_pred2, columns=['negative','positive','neutral'])\n",
    "print(xdum_pred_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_custom(x,y):\n",
    "    if(((x==0)and(y==1) or (x==1)and(y==0) )):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texti  = \"Driverless cars could also end drunk driving forever and save lives..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_vecs_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_tokens = extract_words(texti)\n",
    "print(del_tokens)\n",
    "del_vecs_w2v = buildWordVector(del_tokens,300)\n",
    "print(del_vecs_w2v)\n",
    "print(len(del_vecs_w2v))\n",
    "# del_pred =model_glove4_amazon.predict(del_vecs_w2v,batch_size=1,verbose=0)\n",
    "# # del_pred = 5*del_pred\n",
    "# print(del_pred)\n",
    "# # del_pred = preprocessing.normalize(del_pred)\n",
    "# # del_pred = softmax(del_pred,axis=1)\n",
    "# print(del_pred)\n",
    "# # del_pred = preprocessing.scale(del_pred)\n",
    "# # print(del_pred)\n",
    "# print(get_scores(del_pred.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(x):\n",
    "    prob_vect = [0,0,0]\n",
    "    blob = tb(x)\n",
    "    for sentence in blob.sentences:\n",
    "        del_tokens = extract_words(str(sentence))\n",
    "        del_vecs_w2v = buildWordVector(del_tokens,300)\n",
    "        del_pred =model_glove2_amazon.predict(del_vecs_w2v,batch_size=1,verbose=0)\n",
    "        del_pred = softmax(del_pred,axis=1)\n",
    "        prob_vect += del_pred\n",
    "        return(list(map(get_lable,del_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMO MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "data['clean_comment'] = data['tokens'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "\n",
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))\n",
    "\n",
    "list_train = [data_train[i:i+10000] for i in range(0,data_train.shape[0],10000)]\n",
    "\n",
    "elmo_train = []\n",
    "\n",
    "for i in range(len(list_train)):\n",
    "    print(i)\n",
    "    x = list_train[i]\n",
    "    elmo_train.append(elmo_vectors(x['clean_comment']))\n",
    "\n",
    "x = elmo_train[0]\n",
    "\n",
    "len(x)\n",
    "\n",
    "elmo_train_new = np.concatenate(elmo_train, axis = 0)\n",
    "\n",
    "elmo_train_new[0]\n",
    "\n",
    "elmo_train_scaled = scale(elmo_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "def find_lan2(x):\n",
    "    y2 = 'nolang'\n",
    "    try:\n",
    "        y2 = detect(x)\n",
    "        if(y2=='en'):\n",
    "            print(\"extracting...\")\n",
    "        else:\n",
    "            return([])\n",
    "    except Exception:\n",
    "        return([])\n",
    "        pass  \n",
    "    return(y2)\n",
    "\n",
    "xinpdata = pd.read_csv(\"total_data.csv\")\n",
    "print(len(xinpdata))\n",
    "\n",
    "pool = Pool(processes=cpu_count()-1) \n",
    "print(\"Starting Pooling...\")\n",
    "results =  pool.map(extract_words, xinpdata['text'])\n",
    "print(\"Pooling Finished...\")\n",
    "pool.close()\n",
    "pool.join()\n",
    "xinpdata['tokens'] = results\n",
    "\n",
    "xinpdata['num_words'] = xinpdata['tokens'].apply(lambda x : len(x))\n",
    "xinpdata = xinpdata[xinpdata.num_words > 0]\n",
    "\n",
    "x_train = np.array(xinpdata.tokens)\n",
    "n_dim = 300\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in map(lambda x: x, x_train)])\n",
    "\n",
    "dum_pred = model_amazon.predict(train_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "train_prediction = list(map(get_lable,dum_pred))\n",
    "xinpdata['amazon_result'] = train_prediction\n",
    "\n",
    "dum_pred = model_bing.predict(train_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "train_prediction = list(map(get_lable,dum_pred))\n",
    "xinpdata['bing_result'] = train_prediction\n",
    "\n",
    "dum_pred = model_google.predict(train_vecs_w2v,batch_size=32,verbose=1)\n",
    "dum_pred = softmax(dum_pred,axis=1)\n",
    "train_prediction = list(map(get_lable,dum_pred))\n",
    "xinpdata['google_result'] = train_prediction\n",
    "\n",
    "xinpdata['unmetric_result'] = xinpdata['sentiment'].apply(lambda x : unmetric_lable(x))\n",
    "set(xinpdata.unmetric_result)\n",
    "\n",
    "xinpdata['common'] = list(map(compare_apis,xinpdata['amazon_result'],xinpdata['bing_result'],xinpdata['google_result'],xinpdata['unmetric_result']))\n",
    "xinpdata2 = xinpdata[xinpdata.common==1]\n",
    "print(len(xinpdata2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_w2vec['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
